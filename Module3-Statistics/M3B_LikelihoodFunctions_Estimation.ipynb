{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Basic Probability and Statistics - Likelihood Functions and Estimation\n",
    "<html>\n",
    "    <summary></summary>\n",
    "         <div> <p></p> </div>\n",
    "         <div style=\"font-size: 20px; width: 800px;\"> \n",
    "              <h1>\n",
    "               <left>Intro to Basic Probability and Statistics: Likelihood Functions and Estimation.</left>\n",
    "              </h1>\n",
    "              <p><left>============================================================================</left> </p>\n",
    "<pre>Course: BIOM 421, Spring 2024\n",
    "Instructor: Brian Munsky\n",
    "Authors: Huy Vo, Ania Baetica, Brian Munsky\n",
    "Contact Info: munsky@colostate.edu\n",
    "</pre>\n",
    "         </div>\n",
    "    </p>\n",
    "\n",
    "</html>\n",
    "\n",
    "<details>\n",
    "  <summary>Copyright info</summary>\n",
    "\n",
    "```\n",
    "Copyright 2024 Brian Munsky\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "\n",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "\n",
    "3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "```\n",
    "<details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood functions\n",
    "\n",
    "Likelihood functions serve as fundamental tools for quantifying the plausibility of different parameter values given observed data. Likelihood functions form the backbone of various statistical methods, including maximum likelihood estimation (MLE), Bayesian inference, and hypothesis testing.\n",
    "\n",
    "Likelihood functions provide a formal framework for evaluating how well different parameter values explain the observed data. By assessing the likelihood of observing the data under different parameter settings, we can infer the most plausible values for the parameters governing the underlying statistical model. This process of parameter estimation is essential for making predictions, testing hypotheses, and understanding the underlying mechanisms driving observed phenomena.\n",
    "\n",
    "In this introduction, we will explore the concept of likelihood functions, their role in parameter estimation, and their practical applications in statistical analysis. We will delve into the principles of maximum likelihood estimation, which seeks to find the parameter values that maximize the likelihood function. Later in the course, we will also explore Bayesian inference, which incorporates prior knowledge about the parameters into the estimation process. \n",
    "\n",
    "Additionally, we will discuss the importance of likelihood-based approaches in biology and medicine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Definition of Likelihood function\n",
    "The likelihood function is a fundamental concept in statistical inference that quantifies the plausibility of different parameter values given observed data. Let $\\theta$ denote the parameter of interest in a statistical model, and let ($x_1, x_2, \\ldots, x_n$) represent a sample of $n$ random variables sampled from the population. \n",
    "\n",
    "The likelihood function, denoted by  $L(\\theta) \\equiv L(\\theta | x_1, x_2, \\ldots, x_n) = \\text{Prob}(x_1, x_2, \\ldots, x_n | \\theta ) $, is defined as the joint probability density function (PDF) or probability mass function (PMF) of the observed data when considered as a function of the parameter $\\theta$. \n",
    "\n",
    "Under the assumption of that  ($x_1, x_2, \\ldots, x_n$) are **independent and identically distributed** (i.i.d.) random variables, then the likelihood can be expressed:\n",
    "\n",
    "$L(\\theta) = \\prod_{i=1}^n \\text{Prob}(x_i|\\theta)$\n",
    "\n",
    "When dealing with products of many numbers, it is common practice to describe likelihood functions in terms of their logarthms:\n",
    "\n",
    "$\\log L(\\theta) = \\sum_{i=1}^n \\log \\text{Prob}(x_i|\\theta)$\n",
    "\n",
    "The likelihood function provides a measure of how well different parameter values explain the observed data. Intuitively, parameter values that result in a higher likelihood are considered more plausible given the observed data. Therefore, the likelihood function forms the basis for parameter estimation techniques such as maximum likelihood estimation (MLE) and Bayesian inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Example\n",
    "\n",
    "Let $X_1, X_2, \\ldots, X_n$ be a sample of $n$ independent and identically distributed random variables from a Gaussian (normal) distribution with mean $\\mu$ and variance $\\sigma^2$. The probability density function (PDF) of a single observation $X_i$ is given by:\n",
    "\n",
    "$$\n",
    "f(x_i \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "The likelihood function for the entire sample, denoted $L(\\mu, \\sigma^2 \\mid x_1, x_2, \\ldots, x_n)$, is the product of the PDFs of the individual observations, since the observations are independent. Therefore, the likelihood function is given by:\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2 \\mid x_1, x_2, \\ldots, x_n) = \\prod_{i=1}^{n} f(x_i \\mid \\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "To simplify calculations and avoid numerical underflow or overflow, it's common practice to work with the logarithm of the likelihood function. Taking the logarithm of both sides of the likelihood function yields:\n",
    "\n",
    "$$\n",
    "\\log L(\\mu, \\sigma^2 \\mid x_1, x_2, \\ldots, x_n) = \\sum_{i=1}^{n} \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "This logarithm of the likelihood function is often used in practice for parameter estimation using maximum likelihood estimation (MLE) or Bayesian inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the MLE by plotting the likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of maximum likelihood estimation for gaussian distribution\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Generate some random data\n",
    "np.random.seed(0)\n",
    "true_mu = 5\n",
    "true_sigma = 2\n",
    "N = 100\n",
    "data = np.random.normal(true_mu, true_sigma, N)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Define a function to compute the log likelihood of the data given a mean and standard deviation\n",
    "def log_likelihood(data, mu, sigma):\n",
    "    return np.sum(stats.norm.logpdf(data, mu, sigma))\n",
    "\n",
    "# Make a contour plot of the log likelihood function vs. mu and sigma\n",
    "mu_vals = np.linspace(3, 7, 200)\n",
    "sigma_vals = np.linspace(1, 3, 200)\n",
    "mu_grid, sigma_grid = np.meshgrid(mu_vals, sigma_vals)\n",
    "log_likelihood_grid = np.zeros(mu_grid.shape)\n",
    "for i in range(mu_vals.size):\n",
    "    for j in range(sigma_vals.size):\n",
    "        log_likelihood_grid[j, i] = log_likelihood(data, mu_vals[i], sigma_vals[j])\n",
    "\n",
    "cbar = ax[1].contour(mu_grid, sigma_grid, log_likelihood_grid, 100)\n",
    "# Place a star at the true mu and sigma\n",
    "ax[1].plot(true_mu, true_sigma, 'r*', markersize=15)\n",
    "# Place crosshairs at the maximum likelihood estimate\n",
    "max_likelihood_idx = np.unravel_index(np.argmax(log_likelihood_grid), log_likelihood_grid.shape)\n",
    "ax[1].plot(mu_vals[max_likelihood_idx[1]], sigma_vals[max_likelihood_idx[0]], 'g+', markersize=15)\n",
    "\n",
    "# Add a colorbar and labels\n",
    "fig.colorbar(cbar, ax=ax[1])\n",
    "ax[1].set_xlabel('mu')\n",
    "ax[1].set_ylabel('sigma')\n",
    "ax[1].set_title('Log likelihood of the data')\n",
    "ax[1].legend(['True parameters', 'Maximum likelihood estimate'])\n",
    "\n",
    "# Plot a histogram of the data\n",
    "ax[0].hist(data, bins=20, density=True)\n",
    "ax[0].set_title('Histogram of the data')\n",
    "# add the true density\n",
    "x = np.linspace(0, 10, 100)\n",
    "ax[0].plot(x, stats.norm.pdf(x, true_mu, true_sigma), 'r', linewidth=2)\n",
    "ax[0].legend(['True density', 'Histogram of the data'])\n",
    "# Add the estimated density to the histogram in ax[0]\n",
    "ax[0].plot(x, stats.norm.pdf(x, mu_vals[max_likelihood_idx[1]], sigma_vals[max_likelihood_idx[0]]), 'k--', linewidth=2)\n",
    "ax[0].legend(['True density', 'Estimated density', 'Histogram of the data'])\n",
    "plt.show()\n",
    "\n",
    "# Print the true and estimated parameters\n",
    "print('True mu: ', true_mu)\n",
    "print('True sigma: ', true_sigma)\n",
    "print('Estimated mu (via plotting): ', mu_vals[max_likelihood_idx[1]])\n",
    "print('Estimated sigma: (via plotting)', sigma_vals[max_likelihood_idx[0]])\n",
    "\n",
    "# Print the sample mean and standard deviation\n",
    "print('Sample mean: ', np.mean(data))\n",
    "print('Sample standard deviation: ', np.std(data, ddof=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the MLE through an optimization search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's use an optimization algorithm to find the maximum likelihood estimate\n",
    "\n",
    "# Define a function to compute the negative log likelihood for each set of parameters\n",
    "def neg_log_likelihood(params, data):\n",
    "    mu, sigma = params\n",
    "    # Return the minus log likelihood (which we will want to minimize)\n",
    "    return -np.sum(stats.norm.logpdf(data, mu, sigma))\n",
    "\n",
    "# Use the minimize function to find the maximum likelihood estimate\n",
    "result = minimize(neg_log_likelihood, [3, 3], args=(data,))\n",
    "\n",
    "# Now let's compare the results to the previous estimate\n",
    "# Print the true and estimated parameters\n",
    "print('True mu: ', true_mu)\n",
    "print('True sigma: ', true_sigma)\n",
    "print('Estimated mu (via plotting): ', mu_vals[max_likelihood_idx[1]])\n",
    "print('Estimated sigma: (via plotting)', sigma_vals[max_likelihood_idx[0]])\n",
    "print('Estimated mu (via optimization): ', result.x[0])\n",
    "print('Estimated sigma (via optimization): ', result.x[1])\n",
    "\n",
    "# Print the sample mean and standard deviation\n",
    "print('Sample mean: ', np.mean(data))\n",
    "print('Sample standard deviation: ', np.std(data, ddof=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation using builtin functions within numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use the built-in function to compute the maximum likelihood estimate\n",
    "mu, sigma = stats.norm.fit(data)\n",
    "print('Estimated mu (via built-in function): ', mu)\n",
    "print('Estimated sigma (via built-in function): ', sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the optimization and builtin routines gave us a result that was **very** close to the sample mean and sample variance.  That wasn't just a coincidence..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Calculation of the MLE for Gaussian Distribution\n",
    "To find the maximum likelihood estimators (MLEs) for $\\mu$ and $\\sigma^2$, we maximize the likelihood function with respect to these parameters. Taking the logarithm of the likelihood function, we have:\n",
    "\n",
    "$$\n",
    "\\log L(\\mu, \\sigma^2 \\mid x_1, x_2, \\ldots, x_n) = \\sum_{i=1}^{n} \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "To find the MLE for $\\mu$, we differentiate the logarithm of the likelihood function with respect to $\\mu$ and set it equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu} \\log L(\\mu, \\sigma^2 \\mid x_1, x_2, \\ldots, x_n) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu) = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\mu$, we find:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\bar{x}\n",
    "$$\n",
    "\n",
    "which is the sample mean.\n",
    "\n",
    "To find the MLE for $\\sigma^2$, we differentiate the logarithm of the likelihood function with respect to $\\sigma^2$ and set it equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\sigma^2} \\log L(\\mu, \\sigma^2 \\mid x_1, x_2, \\ldots, x_n) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\sigma^2$, we find:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 = s^2\n",
    "$$\n",
    "\n",
    "which is the sample variance.\n",
    "\n",
    "**Therefore, the maximum likelihood estimators (MLEs) for the mean and standard deviation of a Gaussian distribution are the sample mean $\\bar{x}$ and the sample standard deviation $s$, respectively.**  These are very easy to compute as we have seen earlier!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice MLE estimation on your own:\n",
    "1) Generate five different random data sets from five different distributions (normal, beta, log-normal, gamma, t)\n",
    "2) Compute the likelihood of each data set assuming a model with each of the five different distributions.\n",
    "3) Optimize the parameters of for each combination of data and distributions (5 x 5 = 25 total optimizations).\n",
    "4) Make a table to present the MLE log-likelihood versus model for each combination.\n",
    "5) Comment on whether or not you could identify which data set came from which model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Intervals\n",
    "\n",
    "In most situations, a simple point estimate of some parameters is not sufficient.  We also need to know how accurate or trustworthy is that estimation.  This is the job of **Confidence Intervals**.\n",
    "\n",
    "Basically, a confidence interval is a region of parameter space that is typically expected to contain the true parameters that you are trying to estimate. For example, if you estimated the interval at a confidence level of $\\alpha = 95\\%$, then would would expect that interval to contain the correct parameters set 95% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics of confidence intervals.\n",
    "\n",
    "To be a little more precise, let $X$ be a random sample from a probability distribution, $P$ that you wish to estimate.  Assume that $P$ depends on some parameters, $\\theta$ that you want to know.  We not only want to find $\\theta$, but we also want to know its confidence interval at some level, $\\alpha$ (e.g., 95%). That is we would like to find an interval ($a(X),b(X)$), where $a(X)$ and $b(X)$ are random variables depending on the data, and we want them to satisfy the relationship,\n",
    "$$ P(a(X) < \\theta < b(X)) = \\alpha, \\text{ for every }\\theta. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example to estimate confidence intervals of Gaussian-distributed data using built in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For common distributions, the confidence intervals for the mean and standard deviation can be computed \n",
    "# using built-in functions. For example, for a Gaussian distribution, we can use the norm.interval function.\n",
    "\n",
    "# Compute the standard error of the mean (for our original data set)\n",
    "sem = np.std(data, ddof=0) / np.sqrt(N)\n",
    "# Compute the standard error of the standard deviation\n",
    "ses = np.std(data, ddof=0) / np.sqrt(2 * (N - 1))\n",
    "\n",
    "# Compute the 95% confidence interval for the mean\n",
    "ci_95_mu = stats.norm.interval(0.95, loc=np.mean(data), scale=sem)\n",
    "# Compute the 95% confidence interval for the standard deviation\n",
    "ci_95_sigma = stats.norm.interval(0.95, loc=np.std(data, ddof=0), scale=ses)\n",
    "\n",
    "# Print the results\n",
    "print('95% confidence interval for the mean: ', ci_95_mu)\n",
    "print('95% confidence interval for the standard deviation: ', ci_95_sigma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Distributions with built in functions for confidence intervals\n",
    "Some other distributions have built-in functions to compute confidence intervals for their parameters.\n",
    "For example: \n",
    "\n",
    "* The confidence interval for the mean of a Poisson distribution can be computed using the poisson.interval function.\n",
    "* The confidence interval for the mean of a binomial distribution can be computed using the binom.interval function.\n",
    "* The confidence interval for the mean of an exponential distribution can be computed using the expon.interval function.\n",
    "* The confidence interval for the mean of a gamma distribution can be computed using the gamma.interval function.\n",
    "* The confidence interval for the mean of a beta distribution can be computed using the beta.interval function.\n",
    "* The confidence interval for the mean of a chi-squared distribution can be computed using the chi2.interval function.\n",
    "* The confidence interval for the mean of a t-distribution can be computed using the t.interval function.\n",
    "* The confidence interval for the mean of an F-distribution can be computed using the f.interval function.\n",
    "\n",
    "For other distributions, we can use the bootstrap method to compute confidence intervals for the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating confidence intervals using the bootstrap method.\n",
    "\n",
    "Another approach to estimate confidence intervals is to use the **bootstrap method**. In this method, we\n",
    "resample the data with replacement many times, and compute the maximum likelihood estimate for each\n",
    "resampled data set. We can then use the percentiles of the estimates to compute the confidence intervals.\n",
    "\n",
    "The advantage of the bootstrap method is that it can be used for any distribution, not just the common ones\n",
    "for which built-in functions are available.\n",
    "\n",
    "**Limitations** Some limitations of the bootstrap method are that it can be computationally expensive, and it may not work\n",
    "well for small data sets. However, it can be a useful tool for estimating confidence intervals when the\n",
    "distribution of the maximum likelihood estimates is not well approximated by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use the bootstrap to compute confidence intervals for the mean and standard deviation\n",
    "# for our previous data set. We will use 1000 bootstrap samples to generate the confidence intervals.\n",
    "\n",
    "# Define a function to compute the mean and standard deviation of a bootstrap sample\n",
    "def bootstrap_mean_std(data):\n",
    "    bootstrap_sample = np.random.choice(data, size=data.size, replace=True)\n",
    "    return np.mean(bootstrap_sample), np.std(bootstrap_sample, ddof=0)\n",
    "\n",
    "# Generate 1000 bootstrap samples\n",
    "n_bootstrap = 1000\n",
    "bootstrap_means = np.zeros(n_bootstrap)\n",
    "bootstrap_stds = np.zeros(n_bootstrap)\n",
    "for i in range(n_bootstrap):\n",
    "    bootstrap_means[i], bootstrap_stds[i] = bootstrap_mean_std(data)\n",
    "\n",
    "# Compute the 95% confidence interval for the mean\n",
    "ci_95_mu_bootstrap = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "# Compute the 95% confidence interval for the standard deviation\n",
    "ci_95_sigma_bootstrap = np.percentile(bootstrap_stds, [2.5, 97.5])\n",
    "\n",
    "# Print the results and compare to the previous results\n",
    "print('95% confidence interval for the mean (bootstrap): ', ci_95_mu_bootstrap)\n",
    "print('95% confidence interval for the standard deviation (bootstrap): ', ci_95_sigma_bootstrap)\n",
    "print('95% confidence interval for the mean (analytical): ', ci_95_mu)\n",
    "print('95% confidence interval for the standard deviation (analytical): ', ci_95_sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice working with confidence intervals.\n",
    "1) Generate five different random data sets from five different distributions (normal, beta, log-normal, gamma, t)\n",
    "2) If possible, use the builtin functions to estiamte the confidence intervals for each of the distribution's parameters.\n",
    "3) Also use the bootstrapping approach to estimate the confidence intervals.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
